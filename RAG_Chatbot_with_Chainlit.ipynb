{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOlvDQywnBcCDAzzQAe4kgs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "185037273f4749b58852cf221e8c493a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edc94afcda7844b4a1e4ce2ba26ea253",
              "IPY_MODEL_e4ec16dd192046f295da49d246c43abd",
              "IPY_MODEL_83868f1eb8b6452c8e24efeb81859eba"
            ],
            "layout": "IPY_MODEL_d705ccb301fa4d17aff85965d617211c"
          }
        },
        "edc94afcda7844b4a1e4ce2ba26ea253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66d47f3c31924ccc95ff4bf2b837b15e",
            "placeholder": "​",
            "style": "IPY_MODEL_4cc5c2d8b27a4ccc8504e0940b96cd08",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e4ec16dd192046f295da49d246c43abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edfc437e12864e2daea2df4438798c10",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7abeae8009e441c1a2eca9828d7b648e",
            "value": 2
          }
        },
        "83868f1eb8b6452c8e24efeb81859eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7545e4de66de4dec9a30f0f38fefcb88",
            "placeholder": "​",
            "style": "IPY_MODEL_3c902dcca41744feb457be223044d4f1",
            "value": " 2/2 [00:56&lt;00:00, 25.72s/it]"
          }
        },
        "d705ccb301fa4d17aff85965d617211c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66d47f3c31924ccc95ff4bf2b837b15e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cc5c2d8b27a4ccc8504e0940b96cd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edfc437e12864e2daea2df4438798c10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7abeae8009e441c1a2eca9828d7b648e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7545e4de66de4dec9a30f0f38fefcb88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c902dcca41744feb457be223044d4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pihaf/RAG-Chatbot-with-Chainlit/blob/develop/RAG_Chatbot_with_Chainlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Install dependencies**"
      ],
      "metadata": {
        "id": "u6ZA0NPRlgFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmpqHvZVlcMo",
        "outputId": "9d80ce1f-7198-4594-8937-0e14f169b150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 0.907s\n",
            "\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[33m   ╭────────────────────────────────────────────────────────────────╮\u001b[39m\n",
            "   \u001b[33m│\u001b[39m                                                                \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m      New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m6.14.8\u001b[39m → \u001b[32m10.8.1\u001b[39m       \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m   \u001b[33mChangelog:\u001b[39m \u001b[36mhttps://github.com/npm/cli/releases/tag/v10.8.1\u001b[39m   \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m               Run \u001b[32mnpm install -g npm\u001b[39m to update!                \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m                                                                \u001b[33m│\u001b[39m\n",
            "\u001b[33m   ╰────────────────────────────────────────────────────────────────╯\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chainlit 1.1.304 requires numpy<2.0,>=1.26; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.41.2\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q accelerate==0.31.0\n",
        "!pip install -q langchain==0.2.5\n",
        "!pip install -q langchainhub==0.1.20\n",
        "!pip install -q langchain-chroma==0.1.1\n",
        "!pip install -q langchain-community==0.2.5\n",
        "!pip install -q langchain-openai==0.1.9\n",
        "!pip install -q langchain_huggingface==0.0.3\n",
        "!pip install -q chainlit==1.1.304\n",
        "!pip install -q python-dotenv==1.0.1\n",
        "!pip install -q pypdf==4.2.0\n",
        "!npm install -g localtunnel\n",
        "!pip install -q numpy==1.24.4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Import libraries**"
      ],
      "metadata": {
        "id": "ao9qIrzUl3qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chainlit as cl\n",
        "import torch\n",
        "\n",
        "from chainlit.types import AskFileResponse\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub"
      ],
      "metadata": {
        "id": "1g5FpBsZl8k7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Create a text splitter and a vectorization instance**"
      ],
      "metadata": {
        "id": "lqyK_9YsmXTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size =1000, chunk_overlap=100)\n",
        "\n",
        "embedding = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "3UhJAirDmfel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433bcd28-a896-4394-8149-89abc7567220"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Create a function to process input file**"
      ],
      "metadata": {
        "id": "aljtpzslmn5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file:AskFileResponse):\n",
        "  if file.type == \"text/plain\":\n",
        "    Loader = TextLoader\n",
        "  elif file.type == \"application/pdf\":\n",
        "    Loader = PyPDFLoader\n",
        "\n",
        "  loader = Loader(file.path)\n",
        "  documents = loader.load()\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  for i, doc in enumerate(docs):\n",
        "    doc.metadata[\"source\"] = f\"source_{i}\"\n",
        "  return docs"
      ],
      "metadata": {
        "id": "X7vf3ZskmwsF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Create a function to initialize Chroma database**"
      ],
      "metadata": {
        "id": "CQK1Ts51nxKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector_db(file:AskFileResponse):\n",
        "  docs = process_file(file)\n",
        "  cl.user_session.set(\"docs\", docs)\n",
        "  vector_db = Chroma.from_documents(documents=docs ,\n",
        "                                    embedding=embedding)\n",
        "  return vector_db"
      ],
      "metadata": {
        "id": "NScHN3_Nn5k0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Initialize LLM**"
      ],
      "metadata": {
        "id": "XO9mzEzCoPC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_huggingface_llm(model_name:str = \"lmsys/vicuna-7b-v1.5\",\n",
        "                        max_new_token:int = 512):\n",
        "  nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        "  )\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=nf4_config,\n",
        "    low_cpu_mem_usage=True\n",
        "  )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  model_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=max_new_token,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "  llm = HuggingFacePipeline(\n",
        "    pipeline = model_pipeline,\n",
        "  )\n",
        "  return llm\n",
        "\n",
        "LLM = get_huggingface_llm()"
      ],
      "metadata": {
        "id": "yO_12X21oTBd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "185037273f4749b58852cf221e8c493a",
            "edc94afcda7844b4a1e4ce2ba26ea253",
            "e4ec16dd192046f295da49d246c43abd",
            "83868f1eb8b6452c8e24efeb81859eba",
            "d705ccb301fa4d17aff85965d617211c",
            "66d47f3c31924ccc95ff4bf2b837b15e",
            "4cc5c2d8b27a4ccc8504e0940b96cd08",
            "edfc437e12864e2daea2df4438798c10",
            "7abeae8009e441c1a2eca9828d7b648e",
            "7545e4de66de4dec9a30f0f38fefcb88",
            "3c902dcca41744feb457be223044d4f1"
          ]
        },
        "outputId": "58c46471-9158-443b-9a8a-db7e9eeec28d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "185037273f4749b58852cf221e8c493a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. Initialize welcome message**"
      ],
      "metadata": {
        "id": "VHEuS9TkpPYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "welcome_message = \"\"\" Welcome to the PDF QA! To get started:\n",
        "1. Upload a PDF or text file\n",
        "2. Ask a question about the file\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xdfVYs0epTlo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8. Create function on_chat_start**"
      ],
      "metadata": {
        "id": "W0K0JTiYpaQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cl.on_chat_start\n",
        "async def on_chat_start():\n",
        "  files = None\n",
        "  while files is None:\n",
        "    files = await cl.AskFileMessage(\n",
        "      content = welcome_message,\n",
        "      accept =[\"text/plain\", \"application/pdf\"],\n",
        "      max_size_mb =20,\n",
        "      timeout =180,\n",
        "    ).send()\n",
        "  file = files[0]\n",
        "\n",
        "  msg = cl.Message(content =f\"Processing '{file.name}'...\",\n",
        "                  disable_feedback = True)\n",
        "  await msg.send()\n",
        "\n",
        "  vector_db = await cl.make_async(get_vector_db)(file)\n",
        "\n",
        "  message_history = ChatMessageHistory()\n",
        "  memory = ConversationBufferMemory(\n",
        "    memory_key =\"chat_history\",\n",
        "    output_key =\"answer\",\n",
        "    chat_memory = message_history,\n",
        "    return_messages =True,\n",
        "  )\n",
        "  retriever = vector_db.as_retriever(search_type=\"mmr\",\n",
        "                                    search_kwargs ={'k': 3})\n",
        "  chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm =LLM,\n",
        "    chain_type =\"stuff\",\n",
        "    retriever = retriever,\n",
        "    memory =memory,\n",
        "    return_source_documents = True\n",
        "  )\n",
        "\n",
        "  msg.content = f\"'{file.name}' processed. You can now ask questions!\"\n",
        "  await msg.update()\n",
        "  cl.user_session.set(\"chain\", chain)"
      ],
      "metadata": {
        "id": "qLt5xMmipgS-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**9. Create function on_message**"
      ],
      "metadata": {
        "id": "5-kBv1pvrPFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cl.on_message\n",
        "async def on_message(message:cl.Message):\n",
        "  chain = cl.user_session.get(\"chain\")\n",
        "  cb = cl.AsyncLangchainCallbackHandler()\n",
        "  res = await chain.ainvoke(message.content, callbacks=[cb])\n",
        "  answer = res[\"answer\"]\n",
        "  source_documents = res[\"source_documents\"]\n",
        "  text_elements = []\n",
        "\n",
        "  if source_documents :\n",
        "    for source_idx, source_doc in enumerate(source_documents):\n",
        "      source_name = f\"source_{source_idx}\"\n",
        "      text_elements.append(\n",
        "        cl.Text(content = source_doc.page_content,\n",
        "          name = source_name)\n",
        "      )\n",
        "    source_names = [text_el.name for text_el in text_elements]\n",
        "\n",
        "    if source_names:\n",
        "      answer += f\"\\nSources: {', '.join(source_names)}\"\n",
        "    else:\n",
        "      answer += \"\\nNo sources found\"\n",
        "\n",
        "  await cl.Message(content = answer, elements = text_elements).send()"
      ],
      "metadata": {
        "id": "sMuJxZHcrT5g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**10. Run chainlit app**"
      ],
      "metadata": {
        "id": "TIkFJXxcsuY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chainlit run chatbot_with_chainlit.py --host 0.0.0.0 --port 8000 &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "JBR6bXNFsx6D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**11. Expose localhost into public host using localtunnel**"
      ],
      "metadata": {
        "id": "5cnK9nA5s7Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "IYxxFucctE_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7252a87-3de6-4afe-ee03-80a85fe347df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.34.15.121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lt --port 8000 --subdomain aivn-simple-rag"
      ],
      "metadata": {
        "id": "BqRaj1u3tX4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9136854-0375-439d-d004-8b7c797a1ee3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://aivn-simple-rag.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}